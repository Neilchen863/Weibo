#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import logging
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from fetch import WeiboSpider
from keyword_manager import KeywordManager
from ml_analyzer import MLAnalyzer
import time
import base64
from PIL import Image
import io
import re
import unicodedata

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('weibo_spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = "config.json"
    default_config = {
        "cookie": "",
        "default_pages": 5,
        "min_score": 80,
        "min_likes": 500,  # æ·»åŠ æœ€ä½ç‚¹èµæ•°å‚æ•°
        "download_media": False,
        "max_retries": 3,
        "retry_delay": 5,
        "thread_pool_size": 4,
        "proxy": None
    }
    
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹éƒ½å­˜åœ¨
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
        else:
            config = default_config
            # ä¿å­˜é»˜è®¤é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=4)
        
        return config
    except Exception as e:
        logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return default_config

def save_config(config):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        with open("config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def image_to_base64(image_path, max_size=(300, 300)):
    """
    å°†å›¾ç‰‡è½¬æ¢ä¸ºBase64ç¼–ç å­—ç¬¦ä¸²
    
    å‚æ•°:
    - image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
    - max_size: æœ€å¤§å°ºå¯¸(å®½, é«˜)ï¼Œç”¨äºå‹ç¼©å›¾ç‰‡
    
    è¿”å›:
    - Base64ç¼–ç å­—ç¬¦ä¸²
    """
    try:
        if not os.path.exists(image_path):
            return ""
        
        # æ‰“å¼€å›¾ç‰‡å¹¶è°ƒæ•´å¤§å°ä»¥å‡å°‘æ–‡ä»¶å¤§å°
        with Image.open(image_path) as img:
            # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœæ˜¯RGBAç­‰æ ¼å¼ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                img = img.convert('RGB')
            
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä¿æŒé•¿å®½æ¯”
            img.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # å°†å›¾ç‰‡ä¿å­˜åˆ°å†…å­˜ä¸­
            buffer = io.BytesIO()
            img.save(buffer, format='JPEG', quality=85, optimize=True)
            buffer.seek(0)
            
            # è½¬æ¢ä¸ºBase64
            image_data = buffer.getvalue()
            base64_string = base64.b64encode(image_data).decode('utf-8')
            
            return f"data:image/jpeg;base64,{base64_string}"
    
    except Exception as e:
        logging.warning(f"è½¬æ¢å›¾ç‰‡åˆ°Base64æ—¶å‡ºé”™ {image_path}: {e}")
        return ""

def add_image_data_to_weibos(weibos):
    """
    ä¸ºå¾®åšæ•°æ®æ·»åŠ å›¾ç‰‡çš„Base64ç¼–ç 
    
    å‚æ•°:
    - weibos: å¾®åšæ•°æ®åˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«å›¾ç‰‡æ•°æ®çš„å¾®åšåˆ—è¡¨
    """
    for weibo in weibos:
        image_paths = weibo.get('image_paths', '')
        base64_images = []
        
        if image_paths:
            paths = image_paths.split('|')
            for path in paths:
                if path and os.path.exists(path):
                    base64_data = image_to_base64(path)
                    if base64_data:
                        base64_images.append(base64_data)
        
        # æ·»åŠ Base64å›¾ç‰‡æ•°æ®åˆ°å¾®åšä¿¡æ¯ä¸­
        weibo['image_base64'] = '|'.join(base64_images) if base64_images else ''
        weibo['image_count'] = len(base64_images)
    
    return weibos

def download_filtered_media(spider, filtered_weibos, keyword):
    """
    ä¸ºé€šè¿‡ç­›é€‰çš„é«˜è´¨é‡å¾®åšä¸‹è½½å›¾ç‰‡
    
    å‚æ•°:
    - spider: çˆ¬è™«å®ä¾‹
    - filtered_weibos: ç­›é€‰åçš„å¾®åšåˆ—è¡¨
    - keyword: å…³é”®è¯
    """
    downloaded_count = 0
    for weibo in filtered_weibos:
        if weibo.get('has_images', False):
            image_urls = weibo.get('image_urls', '').split('|')
            image_paths = []
            
            for url in image_urls:
                if url:
                    local_path = spider.download_media(url, 'image', keyword, weibo['weibo_id'])
                    if local_path:
                        image_paths.append(local_path)
                        downloaded_count += 1
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # æ›´æ–°å¾®åšæ•°æ®ä¸­çš„æœ¬åœ°è·¯å¾„ä¿¡æ¯
            weibo['image_paths'] = '|'.join(image_paths)
    
    return downloaded_count

def process_keyword(keyword, spider, ml_analyzer, config, now, keyword_to_type):
    """å¤„ç†å•ä¸ªå…³é”®è¯çš„çˆ¬å–å’Œåˆ†æ"""
    try:
        logging.info(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        # è·å–å…³é”®è¯çš„åˆ†ç±»
        keyword_type = keyword_to_type.get(keyword, "unknown")
        logging.info(f"å…³é”®è¯ '{keyword}' çš„åˆ†ç±»: {keyword_type}")
        
        # è·å–æœç´¢ç»“æœ - æš‚æ—¶å…³é—­åª’ä½“ä¸‹è½½
        results = spider.search_keyword(
            keyword, 
            pages=config["default_pages"], 
            start_page=config["start_page"],
            download_media=False  # å…ˆä¸ä¸‹è½½ï¼Œç­‰ç­›é€‰åå†ä¸‹è½½
        )
        
        if not results:
            logging.warning(f"æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„ç›¸å…³å¾®åš")
            return None
        
        logging.info(f"è·å–åˆ° {len(results)} æ¡å¾®åš")
        
        # åº”ç”¨æœºå™¨å­¦ä¹ åˆ†æ
        logging.info("æ­£åœ¨è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ...")
        analysis_result = ml_analyzer.analyze_weibos(
            results, 
            min_score=config["min_score"],
            min_likes=config["min_likes"],  # ä¼ é€’æœ€ä½ç‚¹èµæ•°å‚æ•°
            min_comments=config["min_comments"] if "min_comments" in config else 0,
            min_forwards=config["min_forwards"] if "min_forwards" in config else 0
        )
        
        if not analysis_result or "filtered_weibos" not in analysis_result:
            logging.warning(f"æœºå™¨å­¦ä¹ åˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœ")
            return None
        
        filtered_results = analysis_result["filtered_weibos"]
        logging.info(f"æœºå™¨å­¦ä¹ åˆ†æåä¿ç•™ {len(filtered_results)} æ¡é«˜è´¨é‡å¾®åš")
        
        # ä¸ºæ¯æ¡å¾®åšæ·»åŠ å…³é”®è¯åˆ†ç±»ä¿¡æ¯
        for weibo in filtered_results:
            weibo['type'] = keyword_type
        
        # å¦‚æœå¯ç”¨äº†åª’ä½“ä¸‹è½½ï¼Œä¸ºç­›é€‰åçš„å¾®åšä¸‹è½½å›¾ç‰‡
        if config["download_media"]:
            logging.info(f"å¼€å§‹ä¸º {keyword} çš„é«˜è´¨é‡å¾®åšä¸‹è½½å›¾ç‰‡...")
            downloaded_count = download_filtered_media(spider, filtered_results, keyword)
            logging.info(f"ä¸ºå…³é”®è¯ '{keyword}' ä¸‹è½½äº† {downloaded_count} å¼ å›¾ç‰‡")
        
        # æ·»åŠ å›¾ç‰‡Base64æ•°æ®åˆ°å¾®åšä¸­
        if config["download_media"]:
            logging.info(f"æ­£åœ¨å¤„ç†å›¾ç‰‡æ•°æ®...")
            filtered_results = add_image_data_to_weibos(filtered_results)
            logging.info(f"å›¾ç‰‡æ•°æ®å¤„ç†å®Œæˆ")
        
        # ä¿å­˜ç»“æœ
        result_dir = "results"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜è¿‡æ»¤åçš„å¾®åšæ•°æ®
        df = pd.DataFrame(filtered_results)
        df = clean_and_reorder_dataframe(df)  # æ¸…ç†å’Œé‡æ–°æ’åˆ—åˆ—
        keyword_file = f"{result_dir}/{keyword}_{now}.csv"
        df.to_csv(keyword_file, index=False, encoding='utf-8-sig')
        logging.info(f"å·²ä¿å­˜è¿‡æ»¤åçš„ç»“æœåˆ° {keyword_file}")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = f"{result_dir}/{keyword}_analysis_{now}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        logging.info(f"å·²ä¿å­˜åˆ†æç»“æœåˆ° {analysis_file}")
        
        # è¾“å‡ºçƒ­é—¨è¯é¢˜
        if "trending_topics" in analysis_result:
            logging.info("\nçƒ­é—¨è¯é¢˜:")
            for topic in analysis_result["trending_topics"]:
                logging.info(f"- {topic['keyword']} (çƒ­åº¦: {topic['score']:.2f}, ç›¸å…³å¾®åšæ•°: {topic['weibo_count']})")
        
        return filtered_results
        
    except Exception as e:
        logging.error(f"å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
        return None

def load_keyword_classifications():
    """
    åŠ è½½å…³é”®è¯åˆ†ç±»ä¿¡æ¯
    
    è¿”å›:
    - å…³é”®è¯åˆ°åˆ†ç±»çš„æ˜ å°„å­—å…¸
    """
    classification_file = "keyword and classification.txt"
    keyword_to_type = {}
    
    try:
        if os.path.exists(classification_file):
            df = pd.read_csv(classification_file, encoding='utf-8')
            # åˆ›å»ºå…³é”®è¯åˆ°åˆ†ç±»çš„æ˜ å°„
            for _, row in df.iterrows():
                keyword = row.iloc[0]  # ç¬¬ä¸€åˆ—æ˜¯å…³é”®è¯
                classification = row.iloc[1]  # ç¬¬äºŒåˆ—æ˜¯åˆ†ç±»
                keyword_to_type[keyword] = classification
            
            logging.info(f"æˆåŠŸåŠ è½½ {len(keyword_to_type)} ä¸ªå…³é”®è¯åˆ†ç±»")
        else:
            logging.warning(f"åˆ†ç±»æ–‡ä»¶ {classification_file} ä¸å­˜åœ¨")
    
    except Exception as e:
        logging.error(f"åŠ è½½å…³é”®è¯åˆ†ç±»æ—¶å‡ºé”™: {e}")
    
    return keyword_to_type

def clean_and_reorder_dataframe(df):
    """
    æ¸…ç†å’Œé‡æ–°æ’åˆ—DataFrameçš„åˆ—
    
    å‚æ•°:
    - df: åŸå§‹DataFrame
    
    è¿”å›:
    - å¤„ç†åçš„DataFrame
    """
    # è¦åˆ é™¤çš„åˆ—ï¼ˆä¿ç•™weibo_idç”¨äºå›¾ç‰‡ç”»å»Šå…³è”ï¼‰
    columns_to_remove = [
        'user_link', 'video_urls', 'image_paths', 'type'
        'video_paths', 'has_images', 'has_videos', 'content_score', 'image_count', 'image_base64'
    ]
    
    # åˆ é™¤æŒ‡å®šåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for col in columns_to_remove:
        if col in df.columns:
            df = df.drop(columns=[col])
    
    # æ•°æ®æ¸…ç†å‡½æ•°
    def clean_text(text):
        if pd.isna(text) or text is None:
            return ''
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        text = str(text)
        
        # ç§»é™¤æˆ–æ›¿æ¢é—®é¢˜å­—ç¬¦
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰
        text = re.sub(r'[\x00-\x08\x0B-\x1F\x7F-\x9F]', '', text)
        
        # è§„èŒƒåŒ– Unicode å­—ç¬¦
        text = unicodedata.normalize('NFKC', text)
        
        # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
        text = re.sub(r'\r?\n', ' ', text)
        
        # æ›¿æ¢åˆ¶è¡¨ç¬¦ä¸ºç©ºæ ¼
        text = re.sub(r'\t', ' ', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        
        # å¤„ç†CSVç‰¹æ®Šå­—ç¬¦ï¼ˆé€—å·ã€å¼•å·ï¼‰
        if ',' in text or '"' in text:
            text = text.replace('"', '""')  # è½¬ä¹‰åŒå¼•å·
        
        return text
    
    # å¯¹æ‰€æœ‰æ–‡æœ¬åˆ—è¿›è¡Œæ¸…ç†
    text_columns = ['content', 'user_name', 'publish_time']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].apply(clean_text)
    
    # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†typeæ”¾åœ¨ç¬¬äºŒåˆ—
    if 'type' in df.columns and 'keyword' in df.columns:
        # è·å–æ‰€æœ‰åˆ—
        all_columns = df.columns.tolist()
        
        # ç§»é™¤keywordå’Œtype
        remaining_columns = [col for col in all_columns if col not in ['keyword', 'type']]
        
        # é‡æ–°æ’åºï¼škeyword, type, ç„¶åæ˜¯å…¶ä»–åˆ—
        new_order = ['keyword', 'type'] + remaining_columns
        df = df[new_order]
    
    return df

def main():
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        
        # åˆ›å»ºç»“æœç›®å½•
        result_dir = "results"
        os.makedirs(result_dir, exist_ok=True)
        
        # åˆ›å»ºå…³é”®è¯ç®¡ç†å™¨
        keyword_manager = KeywordManager()
        
        # ä»æ–‡ä»¶åŠ è½½å…³é”®è¯
        keywords = keyword_manager.load_from_file()
        
        # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤çš„ç¤ºä¾‹å…³é”®è¯
        if not keywords:
            logging.warning("æœªæ‰¾åˆ°å…³é”®è¯æ–‡ä»¶æˆ–æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨ç¤ºä¾‹å…³é”®è¯")
            keywords = [
                "ç¤ºä¾‹å…³é”®è¯1",
                "ç¤ºä¾‹å…³é”®è¯2",
                "ç¤ºä¾‹å…³é”®è¯3",
            ]
            # ä¿å­˜ç¤ºä¾‹å…³é”®è¯åˆ°æ–‡ä»¶
            keyword_manager.add_keywords(keywords)
            keyword_manager.save_to_file()
        
        # å½“å‰æ—¶é—´ï¼Œç”¨äºæ–‡ä»¶å‘½å
        now = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = WeiboSpider()
        
        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
        if config["proxy"]:
            spider.set_proxy(config["proxy"])
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„Cookie
        if config["cookie"]:
            spider.set_cookies(config["cookie"])
            logging.info("å·²ä»é…ç½®æ–‡ä»¶åŠ è½½Cookie")
        # else:
        #     # å¦‚æœæ²¡æœ‰é…ç½®Cookieï¼Œæç¤ºç”¨æˆ·è¾“å…¥
        #     cookie_str = input("è¯·è¾“å…¥å¾®åšCookieï¼ˆå¯é€‰ï¼Œæé«˜çˆ¬å–æˆåŠŸç‡ï¼‰: ")
        #     if cookie_str:
        #         spider.set_cookies(cookie_str)
        #         # ä¿å­˜Cookieåˆ°é…ç½®æ–‡ä»¶
        #         config["cookie"] = cookie_str
        #         save_config(config)
        
        # æç¤ºç”¨æˆ·è¾“å…¥æœ€ä½ç‚¹èµæ•°
        try:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            min_likes = config['min_likes']
            logging.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€ä½ç‚¹èµæ•°é˜ˆå€¼: {min_likes}")
        except ValueError:
            logging.warning(f"é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{config['min_likes']}")
            min_likes = config['min_likes']
        
        logging.info(f"ç­›é€‰é€»è¾‘: åªä¿ç•™ç‚¹èµæ•° >= {min_likes} çš„å¾®åšï¼Œå¹¶å¯¹è¿™äº›å¾®åšè¿›è¡Œç»¼åˆè¯„åˆ†å’Œæ’åº")
        
        # åˆ›å»ºæœºå™¨å­¦ä¹ åˆ†æå™¨å®ä¾‹
        logging.info("æ­£åœ¨åˆå§‹åŒ–æœºå™¨å­¦ä¹ åˆ†æå™¨...")
        ml_analyzer = MLAnalyzer()
        logging.info("æœºå™¨å­¦ä¹ åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åŠ è½½å…³é”®è¯åˆ†ç±»ä¿¡æ¯
        keyword_to_type = load_keyword_classifications()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å…³é”®è¯
        all_results = []
        with ThreadPoolExecutor(max_workers=config["thread_pool_size"]) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_keyword = {
                executor.submit(process_keyword, keyword, spider, ml_analyzer, config, now, keyword_to_type): keyword 
                for keyword in keywords
            }
            
            # æ”¶é›†ç»“æœ
            for future in future_to_keyword:
                keyword = future_to_keyword[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                except Exception as e:
                    logging.error(f"å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°ä¸€ä¸ªåˆå¹¶æ–‡ä»¶
        if all_results:
            df_all = pd.DataFrame(all_results)
            df_all = clean_and_reorder_dataframe(df_all)  # æ¸…ç†å’Œé‡æ–°æ’åˆ—åˆ—
            all_file = f"{result_dir}/all_results_{now}.csv"
            df_all.to_csv(all_file, index=False, encoding='utf-8-sig')
            logging.info(f"\nå·²ä¿å­˜æ‰€æœ‰ç»“æœåˆ° {all_file}")
            logging.info(f"æ€»å…±è·å–åˆ° {len(all_results)} æ¡é«˜è´¨é‡å¾®åš")
            
            # è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡ç”»å»Š
            try:
                from create_simple_gallery import create_simple_gallery
                logging.info("\næ­£åœ¨ç”Ÿæˆå›¾ç‰‡ç”»å»Š...")
                html_file = create_simple_gallery()
                
                if html_file:
                    # è·å–å®Œæ•´è·¯å¾„
                    current_dir = os.getcwd()
                    full_path = os.path.join(current_dir, html_file)
                    
                    # åœ¨ç»ˆç«¯è¾“å‡ºHTMLæ–‡ä»¶ä¿¡æ¯
                    print("\n" + "="*60)
                    print("ğŸ¨ å›¾ç‰‡ç”»å»Šç”Ÿæˆå®Œæˆï¼")
                    print("="*60)
                    print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {html_file}")
                    print(f"ğŸ”— å®Œæ•´è·¯å¾„: {full_path}")
                    print(f"ğŸŒ æµè§ˆå™¨è®¿é—®: file://{full_path}")
                    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
                    print(f"   â€¢ ç›´æ¥åŒå‡»æ‰“å¼€: {html_file}")
                    print(f"   â€¢ æˆ–è¿è¡Œå‘½ä»¤: open {html_file}")
                    print("="*60)
                    
                    # è¯¢é—®æ˜¯å¦ç«‹å³æ‰“å¼€
                    try:
                        user_input = input("\næ˜¯å¦ç«‹å³åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç”»å»Šï¼Ÿ(y/N): ").strip().lower()
                        if user_input in ['y', 'yes', 'æ˜¯']:
                            import webbrowser
                            webbrowser.open(f'file://{full_path}')
                            print("âœ… å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€å›¾ç‰‡ç”»å»Š")
                    except (EOFError, KeyboardInterrupt):
                        print("\nè·³è¿‡æ‰“å¼€ç”»å»Š")
                        
            except ImportError:
                logging.warning("å›¾ç‰‡ç”»å»Šç”Ÿæˆå™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡ç”»å»Šç”Ÿæˆ")
            except Exception as e:
                logging.error(f"ç”Ÿæˆå›¾ç‰‡ç”»å»Šæ—¶å‡ºé”™: {e}")
        else:
            logging.warning("æœªè·å–åˆ°ä»»ä½•ç»“æœï¼Œæ— æ³•ç”Ÿæˆç”»å»Š")
        
    except Exception as e:
        logging.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import logging
import pandas as pd
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from fetch import WeiboSpider
from keyword_manager import KeywordManager
from ml_analyzer import MLAnalyzer
import time
import base64
from PIL import Image
import io
import re
import unicodedata

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('weibo_spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = "config.json"
    default_config = {
        "cookie": "",
        "default_pages": 5,
        "min_score": 80,
        "min_likes": 500,  # æ·»åŠ æœ€ä½ç‚¹èµæ•°å‚æ•°
        "download_media": False,
        "max_retries": 3,
        "retry_delay": 5,
        "thread_pool_size": 4,
        "proxy": None
    }
    
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹éƒ½å­˜åœ¨
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
        else:
            config = default_config
            # ä¿å­˜é»˜è®¤é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=4)
        
        return config
    except Exception as e:
        logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return default_config

def save_config(config):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        with open("config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def image_to_base64(image_path, max_size=(300, 300)):
    """
    å°†å›¾ç‰‡è½¬æ¢ä¸ºBase64ç¼–ç å­—ç¬¦ä¸²
    
    å‚æ•°:
    - image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
    - max_size: æœ€å¤§å°ºå¯¸(å®½, é«˜)ï¼Œç”¨äºå‹ç¼©å›¾ç‰‡
    
    è¿”å›:
    - Base64ç¼–ç å­—ç¬¦ä¸²
    """
    try:
        if not os.path.exists(image_path):
            return ""
        
        # æ‰“å¼€å›¾ç‰‡å¹¶è°ƒæ•´å¤§å°ä»¥å‡å°‘æ–‡ä»¶å¤§å°
        with Image.open(image_path) as img:
            # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœæ˜¯RGBAç­‰æ ¼å¼ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                img = img.convert('RGB')
            
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä¿æŒé•¿å®½æ¯”
            img.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # å°†å›¾ç‰‡ä¿å­˜åˆ°å†…å­˜ä¸­
            buffer = io.BytesIO()
            img.save(buffer, format='JPEG', quality=85, optimize=True)
            buffer.seek(0)
            
            # è½¬æ¢ä¸ºBase64
            image_data = buffer.getvalue()
            base64_string = base64.b64encode(image_data).decode('utf-8')
            
            return f"data:image/jpeg;base64,{base64_string}"
    
    except Exception as e:
        logging.warning(f"è½¬æ¢å›¾ç‰‡åˆ°Base64æ—¶å‡ºé”™ {image_path}: {e}")
        return ""

def add_image_data_to_weibos(weibos):
    """
    ä¸ºå¾®åšæ•°æ®æ·»åŠ å›¾ç‰‡çš„Base64ç¼–ç 
    
    å‚æ•°:
    - weibos: å¾®åšæ•°æ®åˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«å›¾ç‰‡æ•°æ®çš„å¾®åšåˆ—è¡¨
    """
    for weibo in weibos:
        image_paths = weibo.get('image_paths', '')
        base64_images = []
        
        if image_paths:
            paths = image_paths.split('|')
            for path in paths:
                if path and os.path.exists(path):
                    base64_data = image_to_base64(path)
                    if base64_data:
                        base64_images.append(base64_data)
        
        # æ·»åŠ Base64å›¾ç‰‡æ•°æ®åˆ°å¾®åšä¿¡æ¯ä¸­
        weibo['image_base64'] = '|'.join(base64_images) if base64_images else ''
        weibo['image_count'] = len(base64_images)
    
    return weibos

def download_filtered_media(spider, filtered_weibos, keyword):
    """
    ä¸ºé€šè¿‡ç­›é€‰çš„é«˜è´¨é‡å¾®åšä¸‹è½½å›¾ç‰‡
    
    å‚æ•°:
    - spider: çˆ¬è™«å®ä¾‹
    - filtered_weibos: ç­›é€‰åçš„å¾®åšåˆ—è¡¨
    - keyword: å…³é”®è¯
    """
    downloaded_count = 0
    for weibo in filtered_weibos:
        if weibo.get('has_images', False):
            image_urls = weibo.get('image_urls', '').split('|')
            image_paths = []
            
            for url in image_urls:
                if url:
                    local_path = spider.download_media(url, 'image', keyword, weibo['weibo_id'])
                    if local_path:
                        image_paths.append(local_path)
                        downloaded_count += 1
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # æ›´æ–°å¾®åšæ•°æ®ä¸­çš„æœ¬åœ°è·¯å¾„ä¿¡æ¯
            weibo['image_paths'] = '|'.join(image_paths)
    
    return downloaded_count

def parse_weibo_time(time_str, now=None):
    """
    è§£æå¾®åšæ—¶é—´å­—ç¬¦ä¸²ä¸º datetime å¯¹è±¡ã€‚
    æ”¯æŒæ ¼å¼ï¼š'5åˆ†é’Ÿå‰'ã€'ä»Šå¤© 12:34'ã€'æ˜¨å¤© 12:34'ã€'2024-05-23 12:34'ç­‰ã€‚
    """
    if now is None:
        now = datetime.now()
    time_str = str(time_str).strip()
    if not time_str or time_str == 'æœªçŸ¥æ—¶é—´':
        return None
    try:
        if 'åˆ†é’Ÿå‰' in time_str:
            minutes = int(time_str.replace('åˆ†é’Ÿå‰', '').strip())
            return now - timedelta(minutes=minutes)
        elif 'å°æ—¶å‰' in time_str:
            hours = int(time_str.replace('å°æ—¶å‰', '').strip())
            return now - timedelta(hours=hours)
        elif 'ä»Šå¤©' in time_str:
            t = time_str.replace('ä»Šå¤©', '').strip()
            dt = datetime.strptime(t, '%H:%M')
            return now.replace(hour=dt.hour, minute=dt.minute, second=0, microsecond=0)
        elif 'æ˜¨å¤©' in time_str:
            t = time_str.replace('æ˜¨å¤©', '').strip()
            dt = datetime.strptime(t, '%H:%M')
            dt = now.replace(hour=dt.hour, minute=dt.minute, second=0, microsecond=0) - timedelta(days=1)
            return dt
        elif '-' in time_str:
            # å¯èƒ½æ˜¯ '05-23 12:34' æˆ– '2024-05-23 12:34'
            if len(time_str) == 11:  # '05-23 12:34'
                t = f"{now.year}-{time_str}"
                return datetime.strptime(t, '%Y-%m-%d %H:%M')
            elif len(time_str) == 16:  # '2024-05-23 12:34'
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M')
    except Exception:
        pass
    return None

def process_keyword(keyword, spider, ml_analyzer, config, now, keyword_to_type):
    """å¤„ç†å•ä¸ªå…³é”®è¯çš„çˆ¬å–å’Œåˆ†æ"""
    try:
        logging.info(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        # è·å–å…³é”®è¯çš„åˆ†ç±»
        keyword_type = keyword_to_type.get(keyword, "unknown")
        logging.info(f"å…³é”®è¯ '{keyword}' çš„åˆ†ç±»: {keyword_type}")
        
        # è·å–æœç´¢ç»“æœ - å¼ºåˆ¶å…³é—­åª’ä½“ä¸‹è½½
        results = spider.search_keyword(
            keyword, 
            pages=config["default_pages"], 
            start_page=config["start_page"],
            download_media=False  # å¼ºåˆ¶å…³é—­åª’ä½“ä¸‹è½½
        )
        
        if not results:
            logging.warning(f"æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„ç›¸å…³å¾®åš")
            return None
        
        logging.info(f"è·å–åˆ° {len(results)} æ¡å¾®åš")
        
        # ====== ç­›é€‰æœ€è¿‘ä¸¤å¤©çš„å¾®åšï¼ˆä¸å†å¼ºåˆ¶è¦æ±‚è§†é¢‘ï¼‰ ======
        now_dt = datetime.now()
        two_days_ago = now_dt - timedelta(days=2)
        filtered_by_time = []
        for weibo in results:
            dt = parse_weibo_time(weibo.get('publish_time', ''), now=now_dt)
            if dt and dt >= two_days_ago:
                filtered_by_time.append(weibo)
        logging.info(f"ç­›é€‰åå‰©ä½™ {len(filtered_by_time)} æ¡æœ€è¿‘ä¸¤å¤©çš„å¾®åš")
        if not filtered_by_time:
            logging.warning(f"æœ€è¿‘ä¸¤å¤©æ²¡æœ‰å…³é”®è¯ '{keyword}' çš„ç›¸å…³å¾®åš")
            return None
        # ====== åç»­åˆ†æç”¨ filtered_by_time æ›¿æ¢ results ======
        
        # åº”ç”¨æœºå™¨å­¦ä¹ åˆ†æ
        logging.info("æ­£åœ¨è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ...")
        analysis_result = ml_analyzer.analyze_weibos(
            filtered_by_time, 
            min_score=config["min_score"],
            min_likes=config["min_likes"],  # ä¼ é€’æœ€ä½ç‚¹èµæ•°å‚æ•°
            min_comments=config["min_comments"] if "min_comments" in config else 0,
            min_forwards=config["min_forwards"] if "min_forwards" in config else 0
        )
        
        if not analysis_result or "filtered_weibos" not in analysis_result:
            logging.warning(f"æœºå™¨å­¦ä¹ åˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœ")
            return None
        
        filtered_results = analysis_result["filtered_weibos"]
        logging.info(f"æœºå™¨å­¦ä¹ åˆ†æåä¿ç•™ {len(filtered_results)} æ¡é«˜è´¨é‡å¾®åš")
        
        # ä¸ºæ¯æ¡å¾®åšæ·»åŠ å…³é”®è¯åˆ†ç±»ä¿¡æ¯
        for weibo in filtered_results:
            weibo['type'] = keyword_type
        
        # å¦‚æœå¯ç”¨äº†åª’ä½“ä¸‹è½½ï¼Œä¸ºç­›é€‰åçš„å¾®åšä¸‹è½½å›¾ç‰‡ - ç§»é™¤æ­¤åŠŸèƒ½
        if config["download_media"]:
            logging.info(f"åª’ä½“ä¸‹è½½å·²ç¦ç”¨")
        
        # ç§»é™¤å›¾ç‰‡å¤„ç†ç›¸å…³ä»£ç 
        if config["download_media"]:
            logging.info(f"å›¾ç‰‡å¤„ç†å·²ç¦ç”¨")
        
        # ä¿å­˜ç»“æœ
        result_dir = "results"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜è¿‡æ»¤åçš„å¾®åšæ•°æ®
        df = pd.DataFrame(filtered_results)
        df = clean_and_reorder_dataframe(df)  # æ¸…ç†å’Œé‡æ–°æ’åˆ—åˆ—
        # æŒ‰ç‚¹èµé‡é™åºæ’åº
        df = df.sort_values(by='likes', ascending=False)
        keyword_file = f"{result_dir}/{keyword}_{now}.csv"
        df.to_csv(keyword_file, index=False, encoding='utf-8-sig')
        logging.info(f"å·²ä¿å­˜è¿‡æ»¤åçš„ç»“æœåˆ° {keyword_file}")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = f"{result_dir}/{keyword}_analysis_{now}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        logging.info(f"å·²ä¿å­˜åˆ†æç»“æœåˆ° {analysis_file}")
        
        # è¾“å‡ºçƒ­é—¨è¯é¢˜
        if "trending_topics" in analysis_result:
            logging.info("\nçƒ­é—¨è¯é¢˜:")
            for topic in analysis_result["trending_topics"]:
                logging.info(f"- {topic['keyword']} (çƒ­åº¦: {topic['score']:.2f}, ç›¸å…³å¾®åšæ•°: {topic['weibo_count']})")
        
        return filtered_results
        
    except Exception as e:
        logging.error(f"å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
        return None

def load_keyword_classifications():
    """
    åŠ è½½å…³é”®è¯åˆ†ç±»ä¿¡æ¯
    
    è¿”å›:
    - å…³é”®è¯åˆ°åˆ†ç±»çš„æ˜ å°„å­—å…¸
    """
    classification_file = "keyword and classification.txt"
    keyword_to_type = {}
    
    try:
        if os.path.exists(classification_file):
            df = pd.read_csv(classification_file, encoding='utf-8')
            # åˆ›å»ºå…³é”®è¯åˆ°åˆ†ç±»çš„æ˜ å°„
            for _, row in df.iterrows():
                keyword = row.iloc[0]  # ç¬¬ä¸€åˆ—æ˜¯å…³é”®è¯
                classification = row.iloc[1]  # ç¬¬äºŒåˆ—æ˜¯åˆ†ç±»
                keyword_to_type[keyword] = classification
            
            logging.info(f"æˆåŠŸåŠ è½½ {len(keyword_to_type)} ä¸ªå…³é”®è¯åˆ†ç±»")
        else:
            logging.warning(f"åˆ†ç±»æ–‡ä»¶ {classification_file} ä¸å­˜åœ¨")
    
    except Exception as e:
        logging.error(f"åŠ è½½å…³é”®è¯åˆ†ç±»æ—¶å‡ºé”™: {e}")
    
    return keyword_to_type

def clean_and_reorder_dataframe(df):
    """æ¸…ç†å¹¶é‡æ–°æ’åºDataFrame"""
    # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨
    required_columns = ['weibo_id', 'content', 'publish_time', 'reposts_count', 'comments_count', 
                        'attitudes_count', 'post_link', 'video_url', 'video_cover']
    
    # æ·»åŠ ç©ºåˆ—ï¼Œå¦‚æœä¸å­˜åœ¨
    for col in required_columns:
        if col not in df.columns:
            df[col] = ''
    
    # åˆ é™¤ç”¨æˆ·åå­—æ®µ
    if 'user_name' in df.columns:
        df = df.drop(columns=['user_name'])
    
    # ç¡®ä¿post_linkåˆ—éç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨weibo_idç”Ÿæˆ
    if 'weibo_id' in df.columns:
        mask = (df['post_link'].isna()) | (df['post_link'] == '')
        df.loc[mask, 'post_link'] = df.loc[mask, 'weibo_id'].apply(
            lambda x: f'https://weibo.com/detail/{x}'
        )
    
    # æ¸…ç†contentï¼Œä¿ç•™çº¯æ–‡æœ¬
    if 'content' in df.columns:
        # ç§»é™¤HTMLæ ‡ç­¾
        df['content'] = df['content'].astype(str).replace(r'<[^>]*>', '', regex=True)
        # ç§»é™¤å¾®åšç‰¹æ®Šæ ‡è®°å¦‚[è¡¨æƒ…]
        df['content'] = df['content'].replace(r'\[.*?\]', '', regex=True)
        # ç§»é™¤é“¾æ¥
        df['content'] = df['content'].replace(r'http[s]?://\S+', '', regex=True)
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        df['content'] = df['content'].replace(r'\s+', ' ', regex=True).str.strip()
        # ç§»é™¤ç‰¹æ®ŠUnicodeå­—ç¬¦
        df['content'] = df['content'].replace(r'[\u200b-\u200f\u2028-\u202f\u205f-\u206f]', '', regex=True)
        # ç§»é™¤"â€‹â€‹â€‹"ç»“å°¾ï¼ˆè¿™æ˜¯å¾®åšæ–‡æœ¬å¸¸è§çš„ç»“å°¾ï¼‰
        df['content'] = df['content'].replace(r'â€‹+$', '', regex=True)
    
    # é‡æ–°æ’åºåˆ—ï¼Œä¼˜å…ˆæ˜¾ç¤ºé‡è¦ä¿¡æ¯
    ordered_columns = ['keyword'] + required_columns
    
    # åªä¿ç•™åœ¨ordered_columnsä¸­çš„åˆ—
    existing_columns = [col for col in ordered_columns if col in df.columns]
    
    # å¦‚æœæœ‰å…¶ä»–é¢å¤–çš„åˆ—ï¼Œä¹Ÿä¿ç•™
    extra_columns = [col for col in df.columns if col not in ordered_columns]
    
    # åˆå¹¶åˆ—é¡ºåº
    final_columns = existing_columns + extra_columns
    
    return df[final_columns]

def read_keywords(file_path):
    """è¯»å–å…³é”®è¯åˆ—è¡¨æ–‡ä»¶"""
    try:
        # å¦‚æœæ–‡ä»¶è·¯å¾„æ˜¯ "keywords.txt"ï¼Œæ”¹ä¸ºä» "keyword and classification.txt" ä¸­è¯»å–
        if file_path == 'keywords.txt':
            classification_file = "keyword and classification.txt"
            if os.path.exists(classification_file):
                df = pd.read_csv(classification_file, encoding='utf-8')
                # ä»ç¬¬ä¸€åˆ—ï¼ˆå…³é”®è¯åˆ—ï¼‰æå–å…³é”®è¯
                if 'å…³é”®è¯' in df.columns:
                    return df['å…³é”®è¯'].dropna().tolist()
                else:
                    # å¦‚æœæ²¡æœ‰åˆ—åï¼Œå°±å‡è®¾ç¬¬ä¸€åˆ—æ˜¯å…³é”®è¯
                    return df.iloc[:, 0].dropna().tolist()
            else:
                print(f"æ–‡ä»¶ {classification_file} ä¸å­˜åœ¨")
                return []
        else:
            # åŸå§‹çš„ä»æ–‡æœ¬æ–‡ä»¶è¯»å–æ–¹å¼
            with open(file_path, 'r', encoding='utf-8') as f:
                # è¿‡æ»¤æ‰ç©ºè¡Œ
                return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"è¯»å–{file_path}å¤±è´¥: {str(e)}")
        return []

def read_user_urls(file_path):
    """è¯»å–ç”¨æˆ·URLåˆ—è¡¨æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # è¿‡æ»¤æ‰ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except Exception as e:
        print(f"è¯»å–{file_path}å¤±è´¥: {str(e)}")
        return []

def has_video(row):
    # Check video_url field
    if pd.notna(row['video_url']) and row['video_url'].strip() != '':
        return True
    return False

def process_weibo_data(df):
    # Add has_video column and filter
    df['has_video'] = df.apply(has_video, axis=1)
    return df[df['has_video'] == True].drop('has_video', axis=1)

def main():
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è¯»å–å…³é”®è¯åˆ—è¡¨
    keywords = read_keywords('keywords.txt')
    if not keywords:
        logging.error("æœªåœ¨keywords.txtä¸­æ‰¾åˆ°ä»»ä½•å…³é”®è¯")
        return

    logging.info(f"ä»keywords.txtä¸­è¯»å–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")

    # è¯»å–å…³é”®è¯åˆ†ç±»
    keyword_to_type = load_keyword_classifications()
    logging.info(f"åŠ è½½äº† {len(keyword_to_type)} ä¸ªå…³é”®è¯åˆ†ç±»ä¿¡æ¯")

    # è¯»å–ç”¨æˆ·URLåˆ—è¡¨
    user_urls = read_user_urls('user_urls.txt')
    if not user_urls:
        logging.error("user_urls.txtä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç”¨æˆ·URL")
        return

    logging.info(f"ä»user_urls.txtä¸­è¯»å–åˆ° {len(user_urls)} ä¸ªç”¨æˆ·URL")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = WeiboSpider()

    # åˆ›å»ºç»“æœç›®å½•
    result_dir = "results"
    os.makedirs(result_dir, exist_ok=True)

    # å½“å‰æ—¶é—´ï¼Œç”¨äºæ–‡ä»¶å‘½å
    now = datetime.now().strftime("%Y%m%d_%H%M%S")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []

    # å¤„ç†æ¯ä¸ªç”¨æˆ·
    for i, user_url in enumerate(user_urls, 1):
        logging.info(f"\nå¤„ç†ç¬¬ {i}/{len(user_urls)} ä¸ªç”¨æˆ·: {user_url}")
        user_id = spider._extract_user_id(user_url) or f"user_{i}"
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
        for keyword in keywords:
            logging.info(f"\næœç´¢å…³é”®è¯: {keyword}")
            try:
                # çˆ¬å–è¯¥ç”¨æˆ·çš„å¾®åš
                results = spider.search_keyword(
                    user_url=user_url,
                    keyword=keyword,
                    pages=1,  # å›ºå®šä¸º1é¡µ
                    download_media=config["download_media"]
                )
                
                if results:
                    # ä¸ºæ¯æ¡å¾®åšæ·»åŠ ç”¨æˆ·IDå’Œå…³é”®è¯ä¿¡æ¯
                    for result in results:
                        result['user_id'] = user_id
                        result['keyword'] = keyword
                    all_results.extend(results)
                    logging.info(f"æ‰¾åˆ° {len(results)} æ¡åŒ…å«å…³é”®è¯ '{keyword}' çš„å¾®åš")
                else:
                    logging.info(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„å¾®åš")
                
            except Exception as e:
                logging.error(f"å¤„ç†å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {str(e)}")
                continue

    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°CSVæ–‡ä»¶
    if all_results:
        try:
            # è½¬æ¢ä¸ºDataFrame
            df_all = pd.DataFrame(all_results)
            
            # æ¸…ç†å’Œé‡æ–°æ’åºDataFrame
            df_all = clean_and_reorder_dataframe(df_all)
            
            # æ·»åŠ åˆ†ç±»ä¿¡æ¯
            df_all['keyword_type'] = df_all['keyword'].map(keyword_to_type).fillna('other')
            
            # å…ˆæŒ‰å…³é”®è¯åˆ†ç±»æ’åºï¼ˆshowç±»åˆ«ä¼˜å…ˆï¼‰ï¼Œç„¶åæŒ‰ç‚¹èµé‡é™åºæ’åº
            df_all['is_show'] = (df_all['keyword_type'] == 'show').astype(int)
            df_all = df_all.sort_values(by=['is_show', 'attitudes_count'], ascending=[False, False])
            
            # åˆ é™¤è¾…åŠ©æ’åºåˆ—
            if 'is_show' in df_all.columns:
                df_all = df_all.drop(columns=['is_show'])
            if 'keyword_type' in df_all.columns:
                df_all = df_all.drop(columns=['keyword_type'])
            
            # ä¸å†è¿‡æ»¤è§†é¢‘ï¼Œä¿ç•™æ‰€æœ‰å¾®åš
            
            # ä¿å­˜ä¸ºCSV
            output_file = os.path.join(result_dir, f"all_results_{now}.csv")
            df_all.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # åˆ é™¤å¤šä½™å­—æ®µï¼Œåªä¿ç•™æŒ‡å®šå­—æ®µ
            keep_columns = ['keyword', 'weibo_id', 'content', 'publish_time', 'reposts_count', 'comments_count', 'attitudes_count', 'post_link']
            existing_keep_columns = [col for col in keep_columns if col in df_all.columns]
            df_filtered = df_all[existing_keep_columns]
            df_filtered.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            logging.info(f"\nå·²ä¿å­˜æ‰€æœ‰å¾®åšåˆ°: {output_file}")
            logging.info(f"æ€»å…±è·å–åˆ° {len(df_all)} æ¡å¾®åš")

            # è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡ç”»å»Š
            try:
                from create_simple_gallery import create_simple_gallery
                logging.info("\næ­£åœ¨ç”Ÿæˆå›¾ç‰‡ç”»å»Š...")
                html_file = create_simple_gallery()
                
                if html_file:
                    # è·å–å®Œæ•´è·¯å¾„
                    current_dir = os.getcwd()
                    full_path = os.path.join(current_dir, html_file)
                    
                    # åœ¨ç»ˆç«¯è¾“å‡ºHTMLæ–‡ä»¶ä¿¡æ¯
                    print("\n" + "="*60)
                    print("ğŸ¨ å›¾ç‰‡ç”»å»Šç”Ÿæˆå®Œæˆï¼")
                    print("="*60)
                    print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {html_file}")
                    print(f"ğŸ”— å®Œæ•´è·¯å¾„: {full_path}")
                    print(f"ğŸŒ æµè§ˆå™¨è®¿é—®: file://{full_path}")
                    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
                    print(f"   â€¢ ç›´æ¥åŒå‡»æ‰“å¼€: {html_file}")
                    print(f"   â€¢ æˆ–è¿è¡Œå‘½ä»¤: open {html_file}")
                    print("="*60)
                    
                    # è¯¢é—®æ˜¯å¦ç«‹å³æ‰“å¼€
                    try:
                        user_input = input("\næ˜¯å¦ç«‹å³åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç”»å»Šï¼Ÿ(y/N): ").strip().lower()
                        if user_input in ['y', 'yes', 'æ˜¯']:
                            import webbrowser
                            webbrowser.open(f'file://{full_path}')
                            print("âœ… å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€å›¾ç‰‡ç”»å»Š")
                    except (EOFError, KeyboardInterrupt):
                        print("\nè·³è¿‡æ‰“å¼€ç”»å»Š")
                        
            except ImportError:
                logging.warning("å›¾ç‰‡ç”»å»Šç”Ÿæˆå™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡ç”»å»Šç”Ÿæˆ")
            except Exception as e:
                pass  # å¿½ç•¥ç”»å»Šç”Ÿæˆé”™è¯¯
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœåˆ°CSVæ—¶å‡ºé”™: {str(e)}")
    else:
        logging.warning("æœªè·å–åˆ°ä»»ä½•ç»“æœ")

if __name__ == '__main__':
    main()
